\clearpage

\textbf{\underline{Example 1}}

A rock is dropped off a cliff of height $h$. As it falls, we snap a million photographs at random intervals.
On each picture, we measure the distance the rock has fallen. What is the average of all these distances? In other
words, what is the time average of the distance traveled?

Since the rock is dropped from rest, the distance it falls at some time $t$ is given by:

\[
  y(t) = \dfrac{1}{2}gt^{2}
\]

The time it takes to fall the full height, $h$, is:

\[
  h = \dfrac{1}{2}gT^{2}
\]

Solving for the total fall time, $T$:

\begin{align*}
  2h &= gT^{2} \\[1.5ex]
  \dfrac{2h}{g} &= T^{2} \\[1.5ex]
  \sqrt{\dfrac{2h}{g}} &= T
\end{align*}

The time average of $y(t)$ over the time interval of [$0, T$] is:

\begin{align*}
  \langle \, y \, \rangle &= \dfrac{1}{T} \int_{0}^{T} y(t) \, dt \\[1.5ex]
  &= \dfrac{1}{T} \int_{0}^{T} \dfrac{1}{2}gt^2 \, dt
\end{align*}